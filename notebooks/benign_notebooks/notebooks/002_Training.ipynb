{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640946a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED UNSUPERVISED ANOMALY DETECTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading preprocessed data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Load the normalized dataset\u001b[39;00m\n\u001b[0;32m     50\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../results/benign_preprocessed/benign_v5_normalized.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded normalized dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# CRITICAL: Filter out contaminated high-rate traffic\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Based on our analysis, rates > 50,000 pps are suspicious for IoT\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BADALOVKHUSRAV\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\BADALOVKHUSRAV\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\BADALOVKHUSRAV\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\BADALOVKHUSRAV\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyOD imports - comprehensive set\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.loda import LODA\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.so_gaal import SO_GAAL\n",
    "from pyod.models.vae import VAE\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "# For ensemble and evaluation\n",
    "from pyod.models.combination import average, maximization, median\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED UNSUPERVISED ANOMALY DETECTION PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: LOAD AND PREPARE DATA\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 1] Loading preprocessed data...\")\n",
    "\n",
    "# Load the normalized dataset\n",
    "data_path = '../../results/benign_preprocessed/benign_v5_normalized.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"✓ Loaded normalized dataset: {df.shape}\")\n",
    "\n",
    "# CRITICAL: Filter out contaminated high-rate traffic\n",
    "# Based on our analysis, rates > 50,000 pps are suspicious for IoT\n",
    "print(\"\\n[DATA CLEANING] Checking for contaminated samples...\")\n",
    "original_size = len(df)\n",
    "\n",
    "# Since we're using normalized data, we need a different approach\n",
    "# Check if we have a log_rate column (indicates preprocessing was done)\n",
    "if 'log_rate' in df.columns:\n",
    "    # Use statistical threshold instead\n",
    "    print(\"  - Using statistical outlier removal on normalized data\")\n",
    "    # Remove extreme outliers (> 3 std deviations)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    z_scores = np.abs(stats.zscore(df[numeric_cols]))\n",
    "    outlier_mask = (z_scores > 3).any(axis=1)\n",
    "    df_clean = df[~outlier_mask]\n",
    "    print(f\"  ✓ Removed {outlier_mask.sum()} extreme outliers based on z-score > 3\")\n",
    "else:\n",
    "    # If we have the original Rate column\n",
    "    df_clean = df\n",
    "    print(\"  - No filtering applied (working with preprocessed data)\")\n",
    "\n",
    "print(f\"✓ Clean dataset: {df_clean.shape}\")\n",
    "\n",
    "# Select features for anomaly detection\n",
    "# Exclude categorical and identifier columns\n",
    "exclude_cols = ['Protocol Type', 'size_category', 'Header_Length', 'Time_To_Live']\n",
    "feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
    "X = df_clean[feature_cols].values\n",
    "\n",
    "# CRITICAL: Handle inf/nan values\n",
    "print(\"\\n[DATA VALIDATION] Checking for inf/nan values...\")\n",
    "n_inf = np.isinf(X).sum()\n",
    "n_nan = np.isnan(X).sum()\n",
    "print(f\"  - Infinite values found: {n_inf}\")\n",
    "print(f\"  - NaN values found: {n_nan}\")\n",
    "\n",
    "if n_inf > 0 or n_nan > 0:\n",
    "    print(\"  - Replacing inf values with max finite values...\")\n",
    "    # Replace inf with max finite values\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=np.finfo(np.float32).max, \n",
    "                      neginf=np.finfo(np.float32).min)\n",
    "    \n",
    "    # Additional check for very large values\n",
    "    max_val = np.abs(X).max()\n",
    "    if max_val > 1e10:\n",
    "        print(f\"  - Found very large values (max: {max_val:.2e})\")\n",
    "        print(\"  - Clipping to reasonable range...\")\n",
    "        X = np.clip(X, -1e6, 1e6)\n",
    "    \n",
    "    print(\"  ✓ Data cleaned and validated\")\n",
    "\n",
    "# Convert to float32 for memory efficiency\n",
    "X = X.astype(np.float32)\n",
    "print(f\"✓ Features for anomaly detection: {X.shape[1]}\")\n",
    "\n",
    "# Initialize results directory\n",
    "results_dir = '../../results/pyod_advanced/'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: CLASSICAL ANOMALY DETECTION MODELS\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 2] Training Classical Anomaly Detection Models...\")\n",
    "\n",
    "# Define contamination rate based on our cleaned data\n",
    "# Since we removed obvious outliers, use a lower contamination rate\n",
    "contamination = 0.02  # 2% expected anomalies in cleaned data\n",
    "\n",
    "# Initialize models with optimized parameters\n",
    "classical_models = {\n",
    "    'IForest': IForest(\n",
    "        contamination=contamination,\n",
    "        n_estimators=200,\n",
    "        max_features=0.8,\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'LOF': LOF(\n",
    "        contamination=contamination,\n",
    "        n_neighbors=30,\n",
    "        algorithm='auto',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'CBLOF': CBLOF(\n",
    "        contamination=contamination,\n",
    "        n_clusters=8,\n",
    "        alpha=0.9,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'KNN': KNN(\n",
    "        contamination=contamination,\n",
    "        n_neighbors=10,\n",
    "        method='mean',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'HBOS': HBOS(\n",
    "        contamination=contamination,\n",
    "        n_bins=50,\n",
    "        alpha=0.1\n",
    "    ),\n",
    "    'COPOD': COPOD(\n",
    "        contamination=contamination\n",
    "    ),\n",
    "    'ECOD': ECOD(\n",
    "        contamination=contamination,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'LODA': LODA(\n",
    "        contamination=contamination,\n",
    "        n_bins=10,\n",
    "        n_random_cuts=100\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train models and collect results\n",
    "classical_results = {}\n",
    "classical_scores = {}\n",
    "\n",
    "print(\"\\nTraining classical models...\")\n",
    "for name, model in tqdm(classical_models.items(), desc=\"Classical Models\", \n",
    "                       total=len(classical_models)):\n",
    "    print(f\"\\n  Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Fit the model\n",
    "        model.fit(X)\n",
    "        \n",
    "        # Get anomaly scores and predictions\n",
    "        scores = model.decision_scores_\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        n_outliers = np.sum(predictions == 1)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        classical_results[name] = {\n",
    "            'model': model,\n",
    "            'scores': scores,\n",
    "            'predictions': predictions,\n",
    "            'n_outliers': n_outliers,\n",
    "            'outlier_percentage': n_outliers / len(X) * 100,\n",
    "            'train_time': train_time,\n",
    "            'score_mean': np.mean(scores),\n",
    "            'score_std': np.std(scores),\n",
    "            'score_threshold': model.threshold_\n",
    "        }\n",
    "        classical_scores[name] = scores\n",
    "        \n",
    "        print(f\"    ✓ Completed in {train_time:.2f}s\")\n",
    "        print(f\"    ✓ Outliers detected: {n_outliers} ({n_outliers/len(X)*100:.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Error: {str(e)}\")\n",
    "        classical_results[name] = {'error': str(e)}\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: DEEP LEARNING ANOMALY DETECTION\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 3] Training Deep Learning Models...\")\n",
    "\n",
    "# Prepare data for deep learning models\n",
    "# Use fewer features for neural networks to avoid overfitting\n",
    "print(\"  - Applying PCA for dimensionality reduction...\")\n",
    "pca_reducer = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced = pca_reducer.fit_transform(X)\n",
    "print(f\"  ✓ Reduced dimensions for DL: {X.shape[1]} -> {X_reduced.shape[1]}\")\n",
    "\n",
    "# Deep learning models\n",
    "dl_models = {\n",
    "    'AutoEncoder': AutoEncoder(\n",
    "        hidden_neurons=[X_reduced.shape[1], 32, 16, 16, 32, X_reduced.shape[1]],\n",
    "        contamination=contamination,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        dropout_rate=0.2,\n",
    "        l2_regularizer=0.1,\n",
    "        validation_size=0.1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'VAE': VAE(\n",
    "        encoder_neurons=[X_reduced.shape[1], 64, 32],\n",
    "        decoder_neurons=[32, 64, X_reduced.shape[1]],\n",
    "        contamination=contamination,\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        dropout_rate=0.2,\n",
    "        l2_regularizer=0.1,\n",
    "        validation_size=0.1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "dl_results = {}\n",
    "dl_scores = {}\n",
    "\n",
    "print(\"\\nTraining deep learning models...\")\n",
    "for name, model in tqdm(dl_models.items(), desc=\"Deep Learning Models\", \n",
    "                       total=len(dl_models)):\n",
    "    print(f\"\\n  Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Fit the model\n",
    "        model.fit(X_reduced)\n",
    "        \n",
    "        # Get anomaly scores and predictions\n",
    "        scores = model.decision_scores_\n",
    "        predictions = model.predict(X_reduced)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        n_outliers = np.sum(predictions == 1)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        dl_results[name] = {\n",
    "            'model': model,\n",
    "            'scores': scores,\n",
    "            'predictions': predictions,\n",
    "            'n_outliers': n_outliers,\n",
    "            'outlier_percentage': n_outliers / len(X) * 100,\n",
    "            'train_time': train_time,\n",
    "            'score_mean': np.mean(scores),\n",
    "            'score_std': np.std(scores),\n",
    "            'score_threshold': model.threshold_\n",
    "        }\n",
    "        dl_scores[name] = scores\n",
    "        \n",
    "        print(f\"    ✓ Completed in {train_time:.2f}s\")\n",
    "        print(f\"    ✓ Outliers detected: {n_outliers} ({n_outliers/len(X)*100:.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Error: {str(e)}\")\n",
    "        dl_results[name] = {'error': str(e)}\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**classical_results, **dl_results}\n",
    "all_scores = {**classical_scores, **dl_scores}\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: ENSEMBLE METHODS\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 4] Creating Ensemble Models...\")\n",
    "\n",
    "# Collect successful model scores\n",
    "successful_scores = []\n",
    "successful_models = []\n",
    "for name, scores in all_scores.items():\n",
    "    if isinstance(scores, np.ndarray) and len(scores) == len(X):\n",
    "        successful_scores.append(scores.reshape(-1, 1))\n",
    "        successful_models.append(name)\n",
    "\n",
    "if len(successful_scores) >= 3:\n",
    "    # Combine scores\n",
    "    scores_array = np.hstack(successful_scores)\n",
    "    \n",
    "    # Create ensemble scores\n",
    "    ensemble_avg = average(scores_array)\n",
    "    ensemble_max = maximization(scores_array)\n",
    "    ensemble_median = median(scores_array)\n",
    "    \n",
    "    # Determine thresholds\n",
    "    ensemble_results = {\n",
    "        'Average': {\n",
    "            'scores': ensemble_avg,\n",
    "            'threshold': np.percentile(ensemble_avg, 100 - contamination * 100),\n",
    "            'models_used': successful_models\n",
    "        },\n",
    "        'Maximum': {\n",
    "            'scores': ensemble_max,\n",
    "            'threshold': np.percentile(ensemble_max, 100 - contamination * 100),\n",
    "            'models_used': successful_models\n",
    "        },\n",
    "        'Median': {\n",
    "            'scores': ensemble_median,\n",
    "            'threshold': np.percentile(ensemble_median, 100 - contamination * 100),\n",
    "            'models_used': successful_models\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add predictions\n",
    "    for name, data in ensemble_results.items():\n",
    "        data['predictions'] = (data['scores'] > data['threshold']).astype(int)\n",
    "        data['n_outliers'] = np.sum(data['predictions'])\n",
    "        print(f\"✓ Ensemble {name}: {data['n_outliers']} outliers ({data['n_outliers']/len(X)*100:.2f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: VISUALIZATION AND ANALYSIS\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 5] Creating Visualizations...\")\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Training Time Comparison\n",
    "ax1 = axes[0, 0]\n",
    "model_names = [name for name, result in all_results.items() if 'train_time' in result]\n",
    "train_times = [result['train_time'] for name, result in all_results.items() if 'train_time' in result]\n",
    "colors = ['lightblue' if name in classical_models else 'lightcoral' for name in model_names]\n",
    "\n",
    "bars = ax1.bar(range(len(model_names)), train_times, color=colors)\n",
    "ax1.set_xticks(range(len(model_names)))\n",
    "ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Training Time (seconds)')\n",
    "ax1.set_title('Model Training Time Comparison')\n",
    "ax1.legend(['Classical', 'Deep Learning'], loc='upper right')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Outlier Detection Rates\n",
    "ax2 = axes[0, 1]\n",
    "outlier_rates = [result['outlier_percentage'] for name, result in all_results.items() \n",
    "                 if 'outlier_percentage' in result]\n",
    "model_names_outliers = [name for name, result in all_results.items() \n",
    "                       if 'outlier_percentage' in result]\n",
    "\n",
    "bars = ax2.bar(range(len(model_names_outliers)), outlier_rates, color='skyblue')\n",
    "ax2.axhline(y=contamination*100, color='red', linestyle='--', \n",
    "            label=f'Expected ({contamination*100}%)')\n",
    "ax2.set_xticks(range(len(model_names_outliers)))\n",
    "ax2.set_xticklabels(model_names_outliers, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Outlier Detection Rate (%)')\n",
    "ax2.set_title('Outlier Detection Rates by Model')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Score Distribution Comparison (top 5 models)\n",
    "ax3 = axes[1, 0]\n",
    "top_models = list(all_scores.keys())[:5]\n",
    "for i, model_name in enumerate(top_models):\n",
    "    scores = all_scores[model_name]\n",
    "    ax3.hist(scores, bins=50, alpha=0.5, density=True, label=model_name)\n",
    "\n",
    "ax3.set_xlabel('Anomaly Score')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Anomaly Score Distributions (Top 5 Models)')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Correlation Heatmap of Model Scores\n",
    "ax4 = axes[1, 1]\n",
    "if len(successful_scores) >= 3:\n",
    "    score_df = pd.DataFrame(scores_array, columns=successful_models[:len(successful_scores)])\n",
    "    corr_matrix = score_df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0.5, ax=ax4, square=True)\n",
    "    ax4.set_title('Model Score Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Anomaly Visualization using t-SNE\n",
    "print(\"\\nCreating t-SNE visualization...\")\n",
    "if X.shape[0] > 10000:\n",
    "    # Sample for t-SNE if dataset is large\n",
    "    print(\"  - Sampling 10,000 points for t-SNE...\")\n",
    "    sample_idx = np.random.choice(X.shape[0], 10000, replace=False)\n",
    "    X_sample = X[sample_idx]\n",
    "    sample_predictions = {name: result['predictions'][sample_idx] \n",
    "                         for name, result in all_results.items() \n",
    "                         if 'predictions' in result}\n",
    "else:\n",
    "    X_sample = X\n",
    "    sample_predictions = {name: result['predictions'] \n",
    "                         for name, result in all_results.items() \n",
    "                         if 'predictions' in result}\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"  - Running t-SNE (this may take a few minutes)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_jobs=-1)\n",
    "X_tsne = tsne.fit_transform(X_sample[:5000])  # Limit for performance\n",
    "\n",
    "# Create visualization for top 4 models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(list(sample_predictions.items())[:4]):\n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                        c=predictions[:5000], cmap='coolwarm', \n",
    "                        alpha=0.6, s=10)\n",
    "    ax.set_title(f'Anomalies Detected by {model_name}')\n",
    "    ax.set_xlabel('t-SNE Component 1')\n",
    "    ax.set_ylabel('t-SNE Component 2')\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}tsne_anomalies.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: DETAILED OUTLIER ANALYSIS\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 6] Analyzing Detected Outliers...\")\n",
    "\n",
    "# Use ensemble average for final analysis\n",
    "if 'ensemble_results' in locals():\n",
    "    final_predictions = ensemble_results['Average']['predictions']\n",
    "    final_scores = ensemble_results['Average']['scores']\n",
    "else:\n",
    "    # Use best single model\n",
    "    best_model = 'IForest'\n",
    "    final_predictions = all_results[best_model]['predictions']\n",
    "    final_scores = all_results[best_model]['scores']\n",
    "\n",
    "# Get outlier indices\n",
    "outlier_indices = np.where(final_predictions == 1)[0]\n",
    "normal_indices = np.where(final_predictions == 0)[0]\n",
    "\n",
    "# Compare feature statistics\n",
    "feature_comparison = pd.DataFrame()\n",
    "feature_names = df_clean.columns[df_clean.columns.isin(feature_cols)]\n",
    "\n",
    "for feature in feature_names[:15]:  # Top 15 features\n",
    "    outlier_values = df_clean.iloc[outlier_indices][feature]\n",
    "    normal_values = df_clean.iloc[normal_indices][feature]\n",
    "    \n",
    "    feature_comparison = pd.concat([feature_comparison, pd.DataFrame({\n",
    "        'Feature': [feature],\n",
    "        'Normal_Mean': [normal_values.mean()],\n",
    "        'Outlier_Mean': [outlier_values.mean()],\n",
    "        'Difference_%': [(outlier_values.mean() - normal_values.mean()) / \n",
    "                        (normal_values.mean() + 1e-10) * 100],\n",
    "        'Normal_Std': [normal_values.std()],\n",
    "        'Outlier_Std': [outlier_values.std()]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Sort by absolute difference\n",
    "feature_comparison['Abs_Difference_%'] = abs(feature_comparison['Difference_%'])\n",
    "feature_comparison = feature_comparison.sort_values('Abs_Difference_%', ascending=False)\n",
    "\n",
    "# Visualize feature differences\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_comparison.head(10)\n",
    "x = range(len(top_features))\n",
    "plt.bar(x, top_features['Difference_%'], color=['red' if d > 0 else 'blue' \n",
    "                                                for d in top_features['Difference_%']])\n",
    "plt.xticks(x, top_features['Feature'], rotation=45, ha='right')\n",
    "plt.ylabel('Percentage Difference (%)')\n",
    "plt.title('Top 10 Features Distinguishing Outliers from Normal')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 7: SAVE RESULTS AND MODELS\n",
    "# =====================================================================\n",
    "print(\"\\n[STEP 7] Saving Results and Models...\")\n",
    "\n",
    "# Save the best models\n",
    "best_models_to_save = ['IForest', 'ECOD', 'AutoEncoder']\n",
    "for model_name in best_models_to_save:\n",
    "    if model_name in all_results and 'model' in all_results[model_name]:\n",
    "        model_path = f'{results_dir}{model_name}_model.pkl'\n",
    "        joblib.dump(all_results[model_name]['model'], model_path)\n",
    "        print(f\"✓ Saved {model_name} model\")\n",
    "\n",
    "# Save preprocessing pipeline info\n",
    "pipeline_info = {\n",
    "    'contamination': contamination,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': X.shape[1],\n",
    "    'feature_names': feature_cols,\n",
    "    'pca_components': X_reduced.shape[1] if 'X_reduced' in locals() else None,\n",
    "    'models_trained': list(all_results.keys()),\n",
    "    'ensemble_models': successful_models if 'successful_models' in locals() else None\n",
    "}\n",
    "\n",
    "joblib.dump(pipeline_info, f'{results_dir}pipeline_info.pkl')\n",
    "\n",
    "# Create comprehensive report\n",
    "report = []\n",
    "report.append(\"# Advanced Unsupervised Anomaly Detection Report\\n\")\n",
    "report.append(f\"**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "report.append(\"---\\n\\n\")\n",
    "\n",
    "report.append(\"## 1. Dataset Summary\\n\")\n",
    "report.append(f\"- Original samples: {original_size:,}\\n\")\n",
    "report.append(f\"- Samples after cleaning: {len(X):,}\\n\")\n",
    "report.append(f\"- Features used: {X.shape[1]}\\n\")\n",
    "report.append(f\"- Contamination rate: {contamination*100}%\\n\\n\")\n",
    "\n",
    "report.append(\"## 2. Model Performance Summary\\n\\n\")\n",
    "report.append(\"| Model | Training Time | Outliers Detected | Outlier % | Status |\\n\")\n",
    "report.append(\"|-------|--------------|-------------------|-----------|--------|\\n\")\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if 'error' not in result:\n",
    "        report.append(f\"| {name} | {result['train_time']:.2f}s | \"\n",
    "                     f\"{result['n_outliers']:,} | {result['outlier_percentage']:.2f}% | ✓ |\\n\")\n",
    "    else:\n",
    "        report.append(f\"| {name} | - | - | - | ✗ |\\n\")\n",
    "\n",
    "report.append(\"\\n## 3. Top Discriminating Features\\n\\n\")\n",
    "report.append(\"| Feature | Normal Mean | Outlier Mean | Difference % |\\n\")\n",
    "report.append(\"|---------|-------------|--------------|-------------|\\n\")\n",
    "for _, row in feature_comparison.head(10).iterrows():\n",
    "    report.append(f\"| {row['Feature']} | {row['Normal_Mean']:.3f} | \"\n",
    "                 f\"{row['Outlier_Mean']:.3f} | {row['Difference_%']:.1f}% |\\n\")\n",
    "\n",
    "report.append(\"\\n## 4. Ensemble Results\\n\\n\")\n",
    "if 'ensemble_results' in locals():\n",
    "    for name, data in ensemble_results.items():\n",
    "        report.append(f\"- **{name} Ensemble**: {data['n_outliers']:,} outliers \"\n",
    "                     f\"({data['n_outliers']/len(X)*100:.2f}%)\\n\")\n",
    "        report.append(f\"  - Models used: {', '.join(data['models_used'])}\\n\")\n",
    "\n",
    "report.append(\"\\n## 5. Key Findings\\n\\n\")\n",
    "report.append(\"1. **Model Agreement**: \")\n",
    "if 'corr_matrix' in locals():\n",
    "    avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean()\n",
    "    report.append(f\"Average correlation between models: {avg_corr:.3f}\\n\")\n",
    "\n",
    "report.append(\"2. **Outlier Characteristics**: Based on feature analysis, outliers show:\\n\")\n",
    "top_3_features = feature_comparison.head(3)\n",
    "for _, row in top_3_features.iterrows():\n",
    "    direction = \"higher\" if row['Difference_%'] > 0 else \"lower\"\n",
    "    report.append(f\"   - {abs(row['Difference_%']):.0f}% {direction} {row['Feature']}\\n\")\n",
    "\n",
    "report.append(\"\\n## 6. Recommendations\\n\\n\")\n",
    "report.append(\"- Use ensemble methods for production deployment\\n\")\n",
    "report.append(\"- Monitor high-difference features for real-time detection\\n\")\n",
    "report.append(\"- Consider retraining with updated contamination rates\\n\")\n",
    "report.append(\"- Validate detected anomalies with domain experts\\n\")\n",
    "\n",
    "# Save report\n",
    "with open(f'{results_dir}anomaly_detection_report.md', 'w') as f:\n",
    "    f.writelines(report)\n",
    "\n",
    "print(f\"\\n✓ Report saved to: {results_dir}anomaly_detection_report.md\")\n",
    "print(f\"✓ Models saved to: {results_dir}\")\n",
    "print(f\"✓ Visualizations saved to: {results_dir}\")\n",
    "\n",
    "# =====================================================================\n",
    "# FINAL SUMMARY\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANOMALY DETECTION PIPELINE COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Models trained: {len([r for r in all_results.values() if 'error' not in r])}/{len(all_results)}\")\n",
    "print(f\"Best single model: {min(all_results.items(), key=lambda x: abs(x[1].get('outlier_percentage', 100) - contamination*100) if 'outlier_percentage' in x[1] else 100)[0]}\")\n",
    "print(f\"Ensemble models created: {len(ensemble_results) if 'ensemble_results' in locals() else 0}\")\n",
    "print(f\"Total outliers detected (ensemble): {ensemble_results['Average']['n_outliers'] if 'ensemble_results' in locals() else 'N/A'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168f4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
